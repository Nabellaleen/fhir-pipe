{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using fhir-pipe\n",
    "\n",
    "Here is a demo of our pipe which takes in input **mapping rules** and a **SQL database** and output data in the **FHIR format**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "Before starting, you need... mapping and a SQL database! Fortunately we have already provided some rules in the repository [`fhir-mapping`](https://github.com/arkhn/fhir-mapping), so all you have to do is to clone it in the same folder than `fhir-pipe`. Regarding data, well, we have provided a small SQL script `config_cw_local.sql` which will build a very small and fake database following the CW format and which is for illustrative purpose. To build your mock `cw_local` database, [install psql](https://www.postgresql.org/download/) (_you only need the command line tool_) and run:\n",
    "\n",
    "    psql -f config_cw_local.sql "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to load packages from parent\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "import arkhn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precise the project and the FHIR resource you want to fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'Crossway'\n",
    "resource = 'Patient'\n",
    "resource_structure = arkhn.fetcher.get_fhir_resource(project, resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the data from SQL\n",
    "\n",
    "Build the SQL query, and output also the graph of joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ICSF.PATIENT.NOPAT', 'ICSF.PATIENT.NOMPAT', 'ICSF.PATIENT.PREPAT', 'ICSF.PATIENT.PREPATSUITE', 'ICSF.PATCOMP.TELPORT', 'ICSF.PATIENT.SEXE', 'ICSF.PATIENT.DTNAIS', 'ICSF.PATIENT.DECEDE', 'ICSF.PATIENT.DTDECES', 'ICSF.PATADR.ADR1', 'ICSF.PATADR.ADR2', 'ICSF.PATADR.ADR3', 'ICSF.PATADR.ADR4', 'ICSF.PATADR.VILLE', 'ICSF.PATADR.CP', 'ICSF.PAYS.LIBELLE', 'ICSF.PATCOMP.SITUAFAM', 'ICSF.PATCOMP.SITUAFAM', 'ICSF.PATMED.NOMED']\n",
      "[('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATCOMP.NOPAT'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATADR.NOPAT'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATADR.NOPAT'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATADR.NOPAT'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATADR.NOPAT'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATADR.NOPAT'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATADR.NOPAT'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATADR.NOPAT'), ('OneToOne', 'ICSF.PATADR.NOPAYS=ICSF.PAYS.NOPAYS'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATCOMP.NOPAT'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATCOMP.NOPAT'), ('OneToOne', 'ICSF.PATIENT.NOPAT=ICSF.PATMED.NOPAT')]\n",
      "ICSF.PATIENT --- ICSF.PATCOMP\n",
      "ICSF.PATIENT --- ICSF.PATADR\n",
      "ICSF.PATADR --- ICSF.PAYS\n",
      "ICSF.PATIENT --- ICSF.PATMED\n"
     ]
    }
   ],
   "source": [
    "sql_query, squash_rules, graph = arkhn.parser.build_sql_query(project, resource_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `graph` object is intended to help understanding the joins made in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dependency Graph\n",
       "ICSF.PATIENT: [ICSF.PATIENT O2O:(ICSF.PATCOMP,ICSF.PATADR,ICSF.PATMED) O2M:()]\n",
       "ICSF.PATCOMP: [ICSF.PATCOMP O2O:() O2M:()]\n",
       "ICSF.PATADR: [ICSF.PATADR O2O:(ICSF.PAYS) O2M:()]\n",
       "ICSF.PAYS: [ICSF.PAYS O2O:() O2M:()]\n",
       "ICSF.PATMED: [ICSF.PATMED O2O:() O2M:()]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ICSF.PATIENT O2O:(ICSF.PATCOMP,ICSF.PATADR,ICSF.PATMED) O2M:()]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get('ICSF.PATIENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18), []]\n",
      "SELECT ICSF.PATIENT.NOPAT, ICSF.PATIENT.NOMPAT, ICSF.PATIENT.PREPAT, ICSF.PATIENT.PREPATSUITE, ICSF.PATCOMP.TELPORT, ICSF.PATIENT.SEXE, ICSF.PATIENT.DTNAIS, ICSF.PATIENT.DECEDE, ICSF.PATIENT.DTDECES, ICSF.PATADR.ADR1, ICSF.PATADR.ADR2, ICSF.PATADR.ADR3, ICSF.PATADR.ADR4, ICSF.PATADR.VILLE, ICSF.PATADR.CP, ICSF.PAYS.LIBELLE, ICSF.PATCOMP.SITUAFAM, ICSF.PATCOMP.SITUAFAM, ICSF.PATMED.NOMED FROM ICSF.PATIENT LEFT JOIN ICSF.PATCOMP ON ICSF.PATIENT.NOPAT = ICSF.PATCOMP.NOPAT LEFT JOIN ICSF.PATADR ON ICSF.PATIENT.NOPAT = ICSF.PATADR.NOPAT LEFT JOIN ICSF.PAYS ON ICSF.PATADR.NOPAYS = ICSF.PAYS.NOPAYS LEFT JOIN ICSF.PATMED ON ICSF.PATIENT.NOPAT = ICSF.PATMED.NOPAT\n"
     ]
    }
   ],
   "source": [
    "print(squash_rules)\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to launch the sql query and perform the real pipe. If your database is not structured like the one on which the mapping was made, it will fail miserably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the query all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Launching query...')\n",
    "rows = arkhn.sql.run(sql_query)\n",
    "\n",
    "# Fix: replace None values with ''\n",
    "for i, row in enumerate(rows):\n",
    "    rows[i] = [e if e is not None else '' for e in row ]\n",
    "\n",
    "print(len(rows), 'results')\n",
    "\n",
    "# Apply join rule to merge some lines from the same resource\n",
    "rows = arkhn.sql.apply_joins(rows, squash_rules)\n",
    "\n",
    "\n",
    "# Build a fhir object for each resource instance\n",
    "json_rows = []\n",
    "for i, row in enumerate(rows):\n",
    "    if i % 1000 == 0:\n",
    "        progression = round(i / len(rows) * 100, 2)\n",
    "        print('PROGRESS... {} %'.format(progression))\n",
    "    row = list(row)\n",
    "    # The first node has a different structure so we iterate outside the\n",
    "    # dfs_create_fhir function\n",
    "    tree = dict()\n",
    "    for attr in resource_structure['attributes']:\n",
    "        arkhn.parser.dfs_create_fhir(tree, attr, row)\n",
    "    tree, n_leafs = arkhn.parser.clean_fhir(tree)\n",
    "    tree['id'] = int(random.random() * 10e10)\n",
    "    json_rows.append(tree)\n",
    "    # print(json.dumps(tree, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Uncomment to write to file\n",
    "arkhn.parser.write_to_file(json_rows, 'fhir_data/samples.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _or_ Run the query per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "offset = arkhn.log.get('pipe.processing.offset', default=0)\n",
    "\n",
    "for batch_idx, offset, rows in arkhn.sql.batch_run(sql_query, batch_size, offset=offset):\n",
    "    print('Running batch {} offset {}...'.format(batch_idx, offset))\n",
    "    # Rm None values\n",
    "    for i, row in enumerate(rows):\n",
    "        rows[i] = [e if e is not None else '' for e in row ]\n",
    "        \n",
    "    # Apply OneToMany joins\n",
    "    rows = arkhn.sql.apply_joins(rows, squash_rules)\n",
    "    \n",
    "    # Hydrate FHIR objects\n",
    "    json_rows = []\n",
    "    for row in rows:\n",
    "        row = list(row)\n",
    "        # The first node has a different structure so we iterate outside the\n",
    "        # dfs_create_fhir function\n",
    "        tree = dict()\n",
    "        for attr in resource_structure['attributes']:\n",
    "            arkhn.parser.dfs_create_fhir(tree, attr, row)\n",
    "        tree, n_leafs = arkhn.parser.clean_fhir(tree)\n",
    "        tree['id'] = int(random.random() * 10e10)\n",
    "        json_rows.append(tree)\n",
    "        \n",
    "    # Write to file\n",
    "    arkhn.parser.write_to_file(json_rows, 'fhir_data/tmp/samples.{}.json'.format(offset))\n",
    "    \n",
    "    # Log offset to restart in case of a crash\n",
    "    arkhn.log.set('pipe.processing.offset', offset)\n",
    "    \n",
    "# Rm tmp\n",
    "arkhn.log.rm('pipe.processing.offset')\n",
    "\n",
    "print('Merging batches...', end='')\n",
    "with open('fhir_data/samples.json', 'wb') as merged_file:\n",
    "    for batch_filename in glob.glob('fhir_data/tmp/samples.*.json'):\n",
    "        with open(batch_filename, 'rb') as batch_file:\n",
    "            shutil.copyfileobj(batch_file, merged_file)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
